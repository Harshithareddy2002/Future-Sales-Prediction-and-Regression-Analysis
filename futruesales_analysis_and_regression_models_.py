# -*- coding: utf-8 -*-
"""Futruesales Analysis and Regression Models .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oZmK8E9_ugMo4d8HceXeGTaIsltViSSt
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn import metrics

from google.colab import drive
drive.mount('/content/grive')

# Load dataset
big_mart_data = pd.read_csv('/content/grive/MyDrive/Train.csv')

big_mart_data.shape

big_mart_data.info()

big_mart_data.isnull().sum()

big_mart_data['Item_Weight'].median()

# Handling missing values of the feild Item_Weight
big_mart_data['Item_Weight'].fillna(big_mart_data['Item_Weight'].median(), inplace=True)

big_mart_data['Outlet_Size'].mode()

# Handling missing values of the feild Outlet_Size
mode_of_Outlet_size = big_mart_data.pivot_table(values='Outlet_Size', columns='Outlet_Type', aggfunc=(lambda x: x.mode()[0]))

print(mode_of_Outlet_size)

miss_values = big_mart_data['Outlet_Size'].isnull()

big_mart_data.loc[miss_values, 'Outlet_Size'] = big_mart_data.loc[miss_values,'Outlet_Type'].apply(lambda x: mode_of_Outlet_size[x])

big_mart_data.isnull().sum()

big_mart_data.describe()

sns.set()

plt.figure(figsize=(6,6))
sns.countplot(x='Item_Fat_Content', data=big_mart_data)
plt.show()

plt.figure(figsize=(30,6))
sns.countplot(x='Item_Type', data=big_mart_data)
plt.show()

plt.figure(figsize=(6,6))
sns.countplot(x='Outlet_Size', data=big_mart_data)
plt.show()

big_mart_data['Item_Fat_Content'].value_counts()

big_mart_data.replace({'Item_Fat_Content': {'low fat':'Low Fat','LF':'Low Fat', 'reg':'Regular'}}, inplace=True)

big_mart_data['Item_Fat_Content'].value_counts()

big_mart_data.head()

# converting catogorical data to numeric data
encoder = LabelEncoder()

big_mart_data['Item_Identifier'] = encoder.fit_transform(big_mart_data['Item_Identifier'])

big_mart_data['Item_Fat_Content'] = encoder.fit_transform(big_mart_data['Item_Fat_Content'])

big_mart_data['Item_Type'] = encoder.fit_transform(big_mart_data['Item_Type'])

big_mart_data['Outlet_Identifier'] = encoder.fit_transform(big_mart_data['Outlet_Identifier'])

big_mart_data['Outlet_Size'] = encoder.fit_transform(big_mart_data['Outlet_Size'])

big_mart_data['Outlet_Location_Type'] = encoder.fit_transform(big_mart_data['Outlet_Location_Type'])

big_mart_data['Outlet_Type'] = encoder.fit_transform(big_mart_data['Outlet_Type'])

big_mart_data.head()

"""Analysis of the data"""

# Distribution of Target Variable (Item_Outlet_Sales)
plt.figure(figsize=(8, 5))
sns.histplot(big_mart_data['Item_Outlet_Sales'], bins=30, kde=True)
plt.title("Distribution of Item_Outlet_Sales")
plt.xlabel("Item_Outlet_Sales")
plt.ylabel("Frequency")
plt.show()

plt.figure(figsize=(12, 6))
sns.heatmap(big_mart_data.corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Feature Correlation Heatmap")
plt.show()

# Sales by Outlet Type
plt.figure(figsize=(10, 5))
sns.boxplot(x='Outlet_Type', y='Item_Outlet_Sales', data=big_mart_data)
plt.xticks(rotation=45)
plt.title("Sales Distribution by Outlet Type")
plt.show()

# Item Visibility vs Sales
plt.figure(figsize=(10, 5))
sns.scatterplot(x='Item_Visibility', y='Item_Outlet_Sales', data=big_mart_data)
plt.title("Item Visibility vs. Sales")
plt.show()

# Outlet Age Analysis
big_mart_data['Outlet_Age'] = 2024 - big_mart_data['Outlet_Establishment_Year']
plt.figure(figsize=(10, 5))
sns.lineplot(x='Outlet_Age', y='Item_Outlet_Sales', data=big_mart_data, ci=None)
plt.title("Outlet Age vs. Sales Trend")
plt.show()

# Print summary statistics
print("Data Summary:\n", big_mart_data.describe())

# Encoding categorical variables
encoder = LabelEncoder()
for col in ['Item_Identifier', 'Item_Fat_Content', 'Item_Type', 'Outlet_Identifier',
            'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']:
    big_mart_data[col] = encoder.fit_transform(big_mart_data[col])

# Feature Engineering - Outlet Age
big_mart_data['Outlet_Age'] = 2024 - big_mart_data['Outlet_Establishment_Year']

# Splitting data
X = big_mart_data.drop(columns='Item_Outlet_Sales', axis=1)
Y = big_mart_data['Item_Outlet_Sales']

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# ----------------------- Random Forest Regressor -----------------------
rf = RandomForestRegressor(n_estimators=300, max_depth=12, min_samples_split=5, random_state=42)
rf.fit(X_train, Y_train)

rf_train_pred = rf.predict(X_train)
rf_test_pred = rf.predict(X_test)

print("Random Forest Train R²:", metrics.r2_score(Y_train, rf_train_pred))
print("Random Forest Test R²:", metrics.r2_score(Y_test, rf_test_pred))

# ----------------------- XGBoost Regressor -----------------------
xgb = XGBRegressor(
    n_estimators=500,  # More trees but with early stopping
    learning_rate=0.05,
    max_depth=5,  # Reduce complexity
    reg_lambda=10,  # More regularization
    subsample=0.8,  # Reduce overfitting
    colsample_bytree=0.8,  # Feature selection
    random_state=42
)

xgb.fit(X_train, Y_train)


xgb_test_pred = xgb.predict(X_test)
print("Improved XGBoost Test R²:", metrics.r2_score(Y_test, xgb_test_pred))


# ----------------------- Feature Importance -----------------------
feature_importances = pd.DataFrame({'Feature': X.columns, 'Importance': rf.feature_importances_})
feature_importances = feature_importances.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(12, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importances, palette="coolwarm")
plt.title("Feature Importance (Random Forest)")
plt.show()

# Dropping specified columns from training and testing data
drop_cols = ['Outlet_Establishment_Year', 'Outlet_Identifier', 'Outlet_Size', 'Outlet_Location_Type']
X_train_filtered = X_train.drop(columns=drop_cols, errors='ignore')
X_test_filtered = X_test.drop(columns=drop_cols, errors='ignore')

# Checking the new shape
print("X_train shape after dropping columns:", X_train_filtered.shape)
print("X_test shape after dropping columns:", X_test_filtered.shape)

# ----------------------- Train Random Forest -----------------------
rf = RandomForestRegressor(n_estimators=300, max_depth=12, min_samples_split=5, random_state=42)
rf.fit(X_train_filtered, Y_train)

rf_train_pred = rf.predict(X_train_filtered)
rf_test_pred = rf.predict(X_test_filtered)

print("Random Forest Train R²:", metrics.r2_score(Y_train, rf_train_pred))
print("Random Forest Test R²:", metrics.r2_score(Y_test, rf_test_pred))

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import r2_score


# Define the model
rf = RandomForestRegressor(random_state=42)

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 300, 500],  # Number of trees
    'max_depth': [10, 15, 20],  # Tree depth
    'min_samples_split': [2, 5, 10],  # Minimum samples to split a node
    'min_samples_leaf': [1, 2, 4],  # Minimum samples in a leaf node
    'max_features': ['sqrt', 'log2']  # Number of features to consider at each split
}

# Use GridSearchCV for hyperparameter tuning
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,
                           cv=3, n_jobs=-1, verbose=2, scoring='r2')

# Fit the model on training data
grid_search.fit(X_train_filtered, Y_train)

# Print best parameters
print("Best Parameters:", grid_search.best_params_)

# Train the best model on full training set
best_rf = grid_search.best_estimator_

# Evaluate on test data
rf_test_pred = best_rf.predict(X_test_filtered)
rf_test_r2 = r2_score(Y_test, rf_test_pred)

print("Optimized Random Forest Test R²:", rf_test_r2)